{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fce5c89",
   "metadata": {},
   "source": [
    "# ML Model Development Best Practices Template\n",
    "\n",
    "This notebook acts as a **top-level run script** and living document for a new machine learning project.\n",
    "All reusable code should live under `src/` (e.g. `src/utils/`), while this notebook orchestrates:\n",
    "\n",
    "1. Environment and dependency management with **Poetry**\n",
    "2. Generating a `requirements.txt` and importing dependencies\n",
    "3. Creating **utility functions** for loading data from XNAT into `src/utils`\n",
    "4. Formatting/organizing data into a **PyTorch `Dataset`/`DataLoader`**\n",
    "5. Exploratory data analysis (EDA) and saving artifacts under `exploratory/`\n",
    "6. Training a model with logging to `logs/`\n",
    "7. Evaluating the model and saving metrics/plots under `results/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328a718",
   "metadata": {},
   "source": [
    "## 1. Project setup and dependency management with Poetry\n",
    "\n",
    "We use **Poetry** to manage dependencies and virtual environments in a reproducible way.\n",
    "\n",
    "### 1.1. Initialize Poetry in the project root\n",
    "\n",
    "Run these commands **once** at the project root (same directory as this notebook):\n",
    "\n",
    "```bash\n",
    "# Initialize a new Poetry project (answer prompts or edit pyproject.toml afterwards)\n",
    "poetry init\n",
    "\n",
    "# Or, to auto-accept defaults:\n",
    "poetry init --name your_project_name --dependency torch --dependency torchvision --dependency \"pydicom\" --dependency \"xnat\" -n\n",
    "```\n",
    "\n",
    "This creates a `pyproject.toml` file that declares your project and dependencies.\n",
    "\n",
    "### 1.2. Installing dependencies and activating the environment\n",
    "\n",
    "After editing `pyproject.toml` as needed:\n",
    "\n",
    "```bash\n",
    "# Install all dependencies into Poetry's managed virtualenv\n",
    "poetry install\n",
    "\n",
    "# Start a shell in the virtualenv\n",
    "poetry shell\n",
    "```\n",
    "\n",
    "Inside the Poetry shell, start Jupyter (or VS Code) so this notebook runs inside the same environment:\n",
    "\n",
    "```bash\n",
    "jupyter lab\n",
    "# or\n",
    "jupyter notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be98ea",
   "metadata": {},
   "source": [
    "## 2. Exporting `requirements.txt` and core imports\n",
    "\n",
    "Even though Poetry is the source of truth, it's often useful to generate a `requirements.txt`\n",
    "for deployment or other tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b330d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Export a requirements.txt from Poetry (run in a terminal, not in Python)\n",
    "# -------------------------------------------------------------------------\n",
    "# In your shell at the project root:\n",
    "# poetry export -f requirements.txt --output requirements.txt --without-hashes\n",
    "\n",
    "# 2.2 Core imports for this notebook\n",
    "# ----------------------------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Local project imports (code will live under src/)\n",
    "project_root = Path(\".\").resolve()\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# After we scaffold utils in later cells, these imports should work:\n",
    "# from utils.xnat_io import get_xnat_connection, fetch_project_metadata\n",
    "# from utils.datasets import XnatImageDataset, build_dataloaders\n",
    "# from utils.training import train_model\n",
    "# from utils.evaluation import evaluate_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a1bf5",
   "metadata": {},
   "source": [
    "## 3. Utility functions for loading data from XNAT (`src/utils`)\n",
    "\n",
    "All reusable I/O and data-wrangling logic should live under `src/utils/`.\n",
    "This notebook will **scaffold** a minimal set of modules:\n",
    "\n",
    "- `src/utils/xnat_io.py` – connecting to XNAT and pulling metadata\n",
    "- `src/utils/datasets.py` – dataset and dataloader utilities\n",
    "- `src/utils/training.py` – training loop and logging utilities\n",
    "- `src/utils/evaluation.py` – evaluation helpers\n",
    "\n",
    "You can then iterate on these modules without bloating the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c642c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaffolded src/utils modules and core directories.\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Create project directories and utility module stubs\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\".\").resolve()\n",
    "\n",
    "# Core directories\n",
    "src_dir = project_root / \"src\"\n",
    "utils_dir = src_dir / \"utils\"\n",
    "logs_dir = project_root / \"logs\"\n",
    "results_dir = project_root / \"results\"\n",
    "exploratory_dir = project_root / \"exploratory\"\n",
    "\n",
    "for d in [src_dir, utils_dir, logs_dir, results_dir, exploratory_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure src and utils are Python packages\n",
    "(src_dir / \"__init__.py\").touch(exist_ok=True)\n",
    "(utils_dir / \"__init__.py\").touch(exist_ok=True)\n",
    "\n",
    "# 3.2 Minimal xnat_io utilities\n",
    "xnat_io_code = '''\"\"\"XNAT I/O utilities.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import os\n",
    "\n",
    "import xnat  # type: ignore\n",
    "\n",
    "\n",
    "def get_xnat_connection(host: str | None = None, username: str | None = None, password: str | None = None):\n",
    "    \"\"\"Create and return an XNAT connection.\"\"\"\n",
    "    host = host or os.environ.get(\"XNAT_HOST\")\n",
    "    username = username or os.environ.get(\"XNAT_USER\")\n",
    "    password = password or os.environ.get(\"XNAT_PASS\")\n",
    "\n",
    "    if host is None or username is None or password is None:\n",
    "        raise ValueError(\"XNAT_HOST, XNAT_USER, XNAT_PASS must be set or passed explicitly.\")\n",
    "\n",
    "    return xnat.connect(host=host, user=username, password=password)\n",
    "\n",
    "\n",
    "def fetch_project_metadata(connection, project_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Fetch basic metadata for a given XNAT project.\"\"\"\n",
    "    if project_id not in connection.projects:\n",
    "        raise KeyError(f\"Project '{project_id}' not found on XNAT.\")\n",
    "\n",
    "    project = connection.projects[project_id]\n",
    "\n",
    "    metadata: Dict[str, Any] = {\n",
    "        \"id\": project.id,\n",
    "        \"name\": project.name,\n",
    "        \"description\": project.description,\n",
    "        \"num_experiments\": len(project.experiments),\n",
    "    }\n",
    "\n",
    "    return metadata\n",
    "'''\n",
    "\n",
    "# 3.3 Minimal dataset/dataloader utilities\n",
    "datasets_code = '''\"\"\"Dataset and DataLoader utilities for model-ready tensors.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class XnatImageDataset(Dataset):\n",
    "    \"\"\"Generic image dataset built from a manifest DataFrame.\n",
    "\n",
    "    Expected columns:\n",
    "    - 'filepath': path to image file on disk\n",
    "    - 'label': integer class index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        manifest: pd.DataFrame,\n",
    "        transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        self.manifest = manifest.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "        if \"filepath\" not in self.manifest or \"label\" not in self.manifest:\n",
    "            raise ValueError(\"Manifest must contain 'filepath' and 'label' columns.\")\n",
    "\n",
    "        if \"class_name\" in self.manifest:\n",
    "            self.classes = sorted(self.manifest[\"class_name\"].unique().tolist())\n",
    "        else:\n",
    "            self.classes = sorted(self.manifest[\"label\"].unique().tolist())\n",
    "\n",
    "        self.labels = self.manifest[\"label\"].tolist()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.manifest.iloc[idx]\n",
    "        img_path = Path(row[\"filepath\"])\n",
    "        label = int(row[\"label\"])\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def build_dataloaders(\n",
    "    train_manifest: pd.DataFrame,\n",
    "    val_manifest: pd.DataFrame,\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 4,\n",
    "    train_transform: Optional[Callable] = None,\n",
    "    val_transform: Optional[Callable] = None,\n",
    "):\n",
    "    \"\"\"Construct PyTorch dataloaders for train/val splits.\"\"\"\n",
    "    train_ds = XnatImageDataset(train_manifest, transform=train_transform)\n",
    "    val_ds = XnatImageDataset(val_manifest, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return {\"train\": train_loader, \"val\": val_loader}\n",
    "'''\n",
    "\n",
    "# 3.4 Training utilities\n",
    "training_code = '''\"\"\"Training utilities with basic logging.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    dataloaders: Dict[str, DataLoader],\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler,\n",
    "    device: torch.device,\n",
    "    num_epochs: int,\n",
    "    logger,\n",
    ") -> Dict[str, list]:\n",
    "    \"\"\"Generic training loop.\"\"\"\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_err\": [], \"val_err\": []}\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "\n",
    "        for xb, yb in dataloaders[\"train\"]:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_correct += (preds == yb).sum().item()\n",
    "            running_total += yb.size(0)\n",
    "\n",
    "        epoch_train_loss = running_loss / running_total\n",
    "        epoch_train_err = 1.0 - (running_correct / running_total)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_correct = 0\n",
    "        val_running_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dataloaders[\"val\"]:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                outputs = model(xb)\n",
    "                loss = criterion(outputs, yb)\n",
    "\n",
    "                val_running_loss += loss.item() * xb.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_running_correct += (preds == yb).sum().item()\n",
    "                val_running_total += yb.size(0)\n",
    "\n",
    "        epoch_val_loss = val_running_loss / val_running_total\n",
    "        epoch_val_err = 1.0 - (val_running_correct / val_running_total)\n",
    "\n",
    "        history[\"train_loss\"].append(epoch_train_loss)\n",
    "        history[\"val_loss\"].append(epoch_val_loss)\n",
    "        history[\"train_err\"].append(epoch_train_err)\n",
    "        history[\"val_err\"].append(epoch_val_err)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(epoch_val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"train_loss={epoch_train_loss:.4f}, val_loss={epoch_val_loss:.4f}, \"\n",
    "            f\"train_err={epoch_train_err:.4f}, val_err={epoch_val_err:.4f}\"\n",
    "        )\n",
    "\n",
    "    return history\n",
    "'''\n",
    "\n",
    "# 3.5 Evaluation utilities\n",
    "evaluation_code = '''\"\"\"Evaluation utilities for classification models.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    dataloaders: Dict[str, DataLoader],\n",
    "    class_names,\n",
    "    device: torch.device,\n",
    "    results_dir: Path,\n",
    "    prefix: str = \"val\",\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Run evaluation and save reports/plots to results_dir.\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloaders[prefix]:\n",
    "            xb = xb.to(device)\n",
    "            outputs = model(xb)\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(yb.numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(class_names))))\n",
    "\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    report_path = results_dir / f\"{prefix}_classification_report.txt\"\n",
    "    with report_path.open(\"w\") as f:\n",
    "        for label, metrics in report.items():\n",
    "            f.write(f\"{label}: {metrics}\\n\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(\n",
    "        xticks=np.arange(len(class_names)),\n",
    "        yticks=np.arange(len(class_names)),\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        ylabel=\"True label\",\n",
    "        xlabel=\"Predicted label\",\n",
    "        title=\"Confusion matrix\",\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig_path = results_dir / f\"{prefix}_confusion_matrix.png\"\n",
    "    fig.savefig(fig_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return {\n",
    "        \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "        \"macro_precision\": report[\"macro avg\"][\"precision\"],\n",
    "        \"macro_recall\": report[\"macro avg\"][\"recall\"],\n",
    "    }\n",
    "'''\n",
    "\n",
    "# Write the files\n",
    "(utils_dir / \"xnat_io.py\").write_text(xnat_io_code)\n",
    "(utils_dir / \"datasets.py\").write_text(datasets_code)\n",
    "(utils_dir / \"training.py\").write_text(training_code)\n",
    "(utils_dir / \"evaluation.py\").write_text(evaluation_code)\n",
    "\n",
    "print(\"Scaffolded src/utils modules and core directories.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e163a",
   "metadata": {},
   "source": [
    "## 4. Formatting and organizing data for the model\n",
    "\n",
    "This section shows how to:\n",
    "\n",
    "- Build a **manifest** (e.g. a `pandas.DataFrame`) with filepaths and labels.\n",
    "- Use `src/utils/datasets.py` to create PyTorch `Dataset`/`DataLoader` objects.\n",
    "\n",
    "In a real project, you would typically generate the manifest by querying XNAT,\n",
    "downloading DICOMs to disk, and converting them to PNG/JPEG or tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4431dcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     22\u001b[39m train_transform = transforms.Compose([\n\u001b[32m     23\u001b[39m     transforms.Resize((\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)),\n\u001b[32m     24\u001b[39m     transforms.ToTensor(),\n\u001b[32m     25\u001b[39m ])\n\u001b[32m     27\u001b[39m val_transform = transforms.Compose([\n\u001b[32m     28\u001b[39m     transforms.Resize((\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)),\n\u001b[32m     29\u001b[39m     transforms.ToTensor(),\n\u001b[32m     30\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m dataloaders = \u001b[43mbuild_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_manifest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_manifest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_manifest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_manifest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain batches:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataloaders[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVal batches:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataloaders[\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/dnaxuser-wustl_mthorpe_adapt/Notebooks/Project_Mgmt_Best_Practices/src/utils/datasets.py:68\u001b[39m, in \u001b[36mbuild_dataloaders\u001b[39m\u001b[34m(train_manifest, val_manifest, batch_size, num_workers, train_transform, val_transform)\u001b[39m\n\u001b[32m     65\u001b[39m train_ds = XnatImageDataset(train_manifest, transform=train_transform)\n\u001b[32m     66\u001b[39m val_ds = XnatImageDataset(val_manifest, transform=val_transform)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m train_loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=num_workers)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m: train_loader, \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m: val_loader}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:350\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    352\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/sampler.py:143\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from utils.datasets import build_dataloaders\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "manifest_path = data_dir / \"manifest.csv\"\n",
    "\n",
    "if manifest_path.exists():\n",
    "    manifest = pd.read_csv(manifest_path)\n",
    "else:\n",
    "    manifest = pd.DataFrame(columns=[\"filepath\", \"label\", \"class_name\"])\n",
    "\n",
    "if not manifest.empty:\n",
    "    frac_train = 0.8\n",
    "    train_manifest = manifest.sample(frac=frac_train, random_state=42)\n",
    "    val_manifest = manifest.drop(train_manifest.index).reset_index(drop=True)\n",
    "    train_manifest = train_manifest.reset_index(drop=True)\n",
    "else:\n",
    "    train_manifest = manifest.copy()\n",
    "    val_manifest = manifest.copy()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataloaders = build_dataloaders(\n",
    "    train_manifest=train_manifest,\n",
    "    val_manifest=val_manifest,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(dataloaders[\"train\"]))\n",
    "print(\"Val batches:\", len(dataloaders[\"val\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3453f9d",
   "metadata": {},
   "source": [
    "## 5. Exploring the data and saving results to `exploratory/`\n",
    "\n",
    "This section performs basic exploratory data analysis:\n",
    "\n",
    "- Class distribution plots\n",
    "- Example images\n",
    "\n",
    "All figures should be saved under `exploratory/` for traceability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246bb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "exploratory_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not train_manifest.empty:\n",
    "    label_counts = Counter(train_manifest[\"class_name\"])\n",
    "\n",
    "    labels = list(label_counts.keys())\n",
    "    values = [label_counts[l] for l in labels]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(labels, values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Training class distribution\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    class_dist_path = exploratory_dir / \"train_class_distribution.png\"\n",
    "    plt.savefig(class_dist_path)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Saved class distribution plot to: {class_dist_path}\")\n",
    "else:\n",
    "    print(\"Manifest is empty; skipping EDA plots.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c2c92",
   "metadata": {},
   "source": [
    "## 6. Training the model and logging to `logs/`\n",
    "\n",
    "We now define a model, configure logging, and call the reusable training\n",
    "loop from `src/utils/training.py`. All epoch-level logs go to a file under `logs/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f2878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "from utils.training import train_model\n",
    "\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_path = logs_dir / f\"training_{timestamp}.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_path),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(\"training\")\n",
    "\n",
    "logger.info(\"Starting training run\")\n",
    "logger.info(f\"Logging to {log_path}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not train_manifest.empty:\n",
    "    num_classes = len(train_manifest[\"label\"].unique())\n",
    "else:\n",
    "    num_classes = 2\n",
    "logger.info(f\"Detected {num_classes} classes\")\n",
    "\n",
    "try:\n",
    "    weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "    model = models.resnet50(weights=weights)\n",
    "except AttributeError:\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=2)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "if not train_manifest.empty:\n",
    "    history = train_model(\n",
    "        model=model,\n",
    "        dataloaders=dataloaders,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        logger=logger,\n",
    "    )\n",
    "else:\n",
    "    logger.warning(\"Manifest is empty; skipping actual training loop.\")\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_err\": [], \"val_err\": []}\n",
    "\n",
    "logger.info(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb1ecf6",
   "metadata": {},
   "source": [
    "## 7. Evaluating the model and saving results to `results/`\n",
    "\n",
    "Finally, we evaluate the trained model on the validation set and save:\n",
    "\n",
    "- Classification report\n",
    "- Confusion matrix plot\n",
    "- Aggregate metrics\n",
    "\n",
    "All artifacts are written under `results/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import evaluate_model\n",
    "\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not train_manifest.empty:\n",
    "    class_names = sorted(train_manifest[\"class_name\"].unique().tolist())\n",
    "    metrics = evaluate_model(\n",
    "        model=model,\n",
    "        dataloaders=dataloaders,\n",
    "        class_names=class_names,\n",
    "        device=device,\n",
    "        results_dir=results_dir,\n",
    "        prefix=\"val\",\n",
    "    )\n",
    "    print(\"Evaluation metrics:\", metrics)\n",
    "else:\n",
    "    print(\"Manifest is empty; skipping evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
